{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0e7971-db9f-4dae-b9a8-08d7d60d3d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.utils import softmax\n",
    "from torch import Tensor\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Dropout,\n",
    "    InstanceNorm1d,\n",
    "    LayerNorm,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def fill_triangular_torch(x):\n",
    "    m = x.shape[0] # should be n * (n+1) / 2\n",
    "    # solve for n\n",
    "    n = int(math.sqrt((0.25 + 2 * m)) - 0.5)\n",
    "    idx = torch.tensor(m - (n**2 - m))\n",
    "    \n",
    "    x_tail = x[idx:]\n",
    "        \n",
    "    return torch.cat([x_tail, torch.flip(x, [0])], 0).reshape(n, n)\n",
    "\n",
    "def fill_diagonal_torch(a, val):\n",
    "    a[..., torch.arange(0, a.shape[0]), torch.arange(0, a.shape[0])] = val\n",
    "    #a[..., torch.arange(0, a.shape[0]).to(device), torch.arange(0, a.shape[0]).to(device)] = val\n",
    "    return a\n",
    "\n",
    "def construct_fisher_matrix_multiple_torch(outputs):\n",
    "    Q = torch.vmap(fill_triangular_torch)(outputs)\n",
    "    # vmap the jnp.diag function for the batch\n",
    "    _diag = torch.vmap(torch.diag)\n",
    "    \n",
    "    middle = _diag(torch.triu(Q) - torch.nn.Softplus()(torch.triu(Q))).to(device)\n",
    "        \n",
    "    padding = torch.zeros(Q.shape).to(device)\n",
    "    \n",
    "    # vmap the fill_diagonal code\n",
    "    L = Q - torch.vmap(fill_diagonal_torch)(padding, middle)\n",
    "\n",
    "    return torch.einsum('...ij,...jk->...ik', L, torch.permute(L, (0, 2, 1)))\n",
    "\n",
    "\n",
    "\n",
    "## ADD IN AN MLP TO GET US TO THE RIGHT DIMENSIONALITY\n",
    "class MLP(Sequential):\n",
    "    def __init__(self, channels: List[int], norm: Optional[str] = None,\n",
    "                 bias: bool = True, dropout: float = 0.):\n",
    "        m = []\n",
    "        for i in range(1, len(channels)):\n",
    "            m.append(Linear(channels[i - 1], channels[i], bias=bias))\n",
    "\n",
    "            if i < len(channels) - 1:\n",
    "                if norm and norm == 'batch':\n",
    "                    m.append(BatchNorm1d(channels[i], affine=True))\n",
    "                elif norm and norm == 'layer':\n",
    "                    m.append(LayerNorm(channels[i], elementwise_affine=True))\n",
    "                elif norm and norm == 'instance':\n",
    "                    m.append(InstanceNorm1d(channels[i], affine=False))\n",
    "                elif norm:\n",
    "                    raise NotImplementedError(\n",
    "                        f'Normalization layer \"{norm}\" not supported.')\n",
    "                m.append(ReLU())\n",
    "                m.append(Dropout(dropout))\n",
    "\n",
    "        super().__init__(*m)\n",
    "\n",
    "\n",
    "\n",
    "class FishnetsAggregation(Aggregation):\n",
    "    r\"\"\"Fishnets aggregation for GNNs\n",
    "\n",
    "    .. math::\n",
    "        \\mathrm{var}(\\mathcal{X}) = \\mathrm{mean}(\\{ \\mathbf{x}_i^2 : x \\in\n",
    "        \\mathcal{X} \\}) - \\mathrm{mean}(\\mathcal{X})^2.\n",
    "\n",
    "    Args:\n",
    "        n_p (int): latent space size\n",
    "        semi_grad (bool, optional): If set to :obj:`True`, will turn off\n",
    "            gradient calculation during :math:`E[X^2]` computation. Therefore,\n",
    "            only semi-gradients are used during backpropagation. Useful for\n",
    "            saving memory and accelerating backward computation.\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_p: int, in_size: int = None, semi_grad: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_p = n_p\n",
    "        \n",
    "        if in_size is None:\n",
    "            in_size = n_p\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.semi_grad = semi_grad\n",
    "        fdim = n_p + ((n_p * (n_p + 1)) // 2)\n",
    "        self.fishnets_dims = fdim\n",
    "        from torch_geometric.nn import Linear\n",
    "        self.lin_1 = Linear(in_size, fdim, bias=True).to(device)\n",
    "        self.lin_2 = Linear(n_p, in_size, bias=True).to(device)\n",
    "\n",
    "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
    "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
    "                dim: int = -2) -> Tensor:\n",
    "        \n",
    "        # GET X TO THE RIGHT DIMENSIONALITY\n",
    "        x = self.lin_1(x)\n",
    "        \n",
    "        # CONSTRUCT SCORE AND FISHER\n",
    "        # the input x will be n_p + (n_p*(n_p + 1) // 2) long\n",
    "        score = x[..., :self.n_p]\n",
    "        fisher = x[..., self.n_p:]\n",
    "        \n",
    "        # reduce the score\n",
    "        score = self.reduce(score, index, ptr, dim_size, dim, reduce='sum')\n",
    "        \n",
    "        # construct the fisher\n",
    "        fisher = construct_fisher_matrix_multiple_torch(fisher)\n",
    "\n",
    "        # sum the fishers\n",
    "        fisher = self.reduce(fisher.reshape(-1, self.n_p**2), \n",
    "                             index, ptr, dim_size, dim, reduce='sum').reshape(-1, self.n_p, self.n_p)\n",
    "        \n",
    "        # add in the prior \n",
    "        fisher += torch.eye(self.n_p).to(device)\n",
    "        \n",
    "        # calculate inverse-dot product\n",
    "        mle = torch.einsum('...jk,...k->...j', torch.linalg.inv(fisher), score)\n",
    "        \n",
    "        # if we decide to bottleneck, send through linear back to node dimensionality\n",
    "        if self.in_size != self.n_p:\n",
    "            mle = self.lin_2(mle)\n",
    "          \n",
    "        return mle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a5dbf6-ae8c-4d04-b2ac-75deef60f9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff59d85-84ad-445f-9ca2-20a24f4b7cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size 14\n"
     ]
    }
   ],
   "source": [
    "n_p = 4\n",
    "embedding_size = n_p + ((n_p * (n_p + 1)) // 2)\n",
    "print(\"embedding size\", embedding_size)\n",
    "\n",
    "#print(embedding_size)\n",
    "mle = FishnetsAggregation(n_p=n_p)(torch.ones((300, n_p)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12a9f6-7d79-4aa2-9148-e0954096d756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Training epoch: 0001: 100%|██████████| 40/40 [00:52<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0002: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0002: 100%|██████████| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3175, Train: 0.7129, Val: 0.6674, Test: 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0003: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0004: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0004: 100%|██████████| 5/5 [00:15<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2996, Train: 0.7534, Val: 0.7072, Test: 0.6812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0005: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0006: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n",
      "Evaluating epoch: 0006: 100%|██████████| 5/5 [00:16<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2885, Train: 0.7627, Val: 0.6956, Test: 0.6728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0007: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0008: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0008: 100%|██████████| 5/5 [00:16<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2865, Train: 0.7741, Val: 0.7466, Test: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0009: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0010: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0010: 100%|██████████| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2870, Train: 0.7867, Val: 0.7598, Test: 0.7228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0011: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0012: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0012: 100%|██████████| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2798, Train: 0.7855, Val: 0.7261, Test: 0.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0013: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0014: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0014: 100%|██████████| 5/5 [00:16<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2764, Train: 0.7801, Val: 0.7304, Test: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0015: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0016: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0016: 100%|██████████| 5/5 [00:16<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2748, Train: 0.7901, Val: 0.7318, Test: 0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0017: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0018: 100%|██████████| 40/40 [00:54<00:00,  1.37s/it]\n",
      "Evaluating epoch: 0018: 100%|██████████| 5/5 [00:16<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2741, Train: 0.7968, Val: 0.7564, Test: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0019: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0020: 100%|██████████| 40/40 [00:53<00:00,  1.33s/it]\n",
      "Evaluating epoch: 0020: 100%|██████████| 5/5 [00:15<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2712, Train: 0.8010, Val: 0.7584, Test: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0021: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0022: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n",
      "Evaluating epoch: 0022: 100%|██████████| 5/5 [00:15<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2713, Train: 0.8005, Val: 0.7637, Test: 0.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0023: 100%|██████████| 40/40 [00:53<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch: 0024:  18%|█▊        | 7/40 [00:13<00:55,  1.69s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
    "from torch.nn import LayerNorm, Linear, ReLU\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import RandomNodeLoader\n",
    "from torch_geometric.nn import DeepGCNLayer, GENConv\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "dataset = PygNodePropPredDataset('ogbn-proteins', root='/data101/makinen/ogbn/')\n",
    "splitted_idx = dataset.get_idx_split()\n",
    "data = dataset[0]\n",
    "data.node_species = None\n",
    "data.y = data.y.to(torch.float)\n",
    "\n",
    "# Initialize features of nodes by aggregating edge features.\n",
    "row, col = data.edge_index\n",
    "data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "\n",
    "# Set split indices to masks.\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    mask[splitted_idx[split]] = True\n",
    "    data[f'{split}_mask'] = mask\n",
    "\n",
    "train_loader = RandomNodeLoader(data, num_parts=40, shuffle=True,\n",
    "                                num_workers=5)\n",
    "\n",
    "\n",
    "test_loader = RandomNodeLoader(data, num_parts=5, num_workers=5)\n",
    "\n",
    "\n",
    "class DeeperGCN(torch.nn.Module):\n",
    "    def __init__(self, n_p, num_layers, hidden_channels=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # need some extra channels for the fisher matrix\n",
    "        fishnets_channels = n_p + ((n_p * (n_p + 1)) // 2)\n",
    "        \n",
    "        if hidden_channels is None:\n",
    "            hidden_channels = n_p\n",
    "\n",
    "        self.node_encoder = Linear(data.x.size(-1), hidden_channels)\n",
    "        self.edge_encoder = Linear(data.edge_attr.size(-1), hidden_channels)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(1, num_layers + 1):\n",
    "            conv = GENConv(hidden_channels, hidden_channels, \n",
    "                           aggr=FishnetsAggregation(in_size=hidden_channels, n_p=n_p),\n",
    "                           t=1.0, learn_t=False, \n",
    "                           num_layers=2, norm='layer')\n",
    "            # output of conv is n_p size\n",
    "            norm = LayerNorm(hidden_channels, elementwise_affine=True)\n",
    "            act = ReLU(inplace=True)\n",
    "\n",
    "            layer = DeepGCNLayer(conv, norm, act, block='res+', dropout=0.1,\n",
    "                                 ckpt_grad=i % 3)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, data.y.size(-1))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "\n",
    "        x = self.layers[0].conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "            #print(\"x\", x.shape)\n",
    "\n",
    "        x = self.layers[0].act(self.layers[0].norm(x))\n",
    "        x = F.dropout(x, p=0.1, training=self.training)\n",
    "\n",
    "        return self.lin(x)\n",
    "\n",
    "FISHNETS_N_P = 5\n",
    "HIDDEN_CHANNELS = 24\n",
    "NUM_LAYERS = 3 # was 28\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DeeperGCN(n_p=FISHNETS_N_P, num_layers=NUM_LAYERS, hidden_channels=HIDDEN_CHANNELS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "evaluator = Evaluator('ogbn-proteins')\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader), position=0)\n",
    "    pbar.set_description(f'Training epoch: {epoch:04d}')\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * int(data.train_mask.sum())\n",
    "        total_examples += int(data.train_mask.sum())\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    y_true = {'train': [], 'valid': [], 'test': []}\n",
    "    y_pred = {'train': [], 'valid': [], 'test': []}\n",
    "\n",
    "    pbar = tqdm(total=len(test_loader), position=0)\n",
    "    pbar.set_description(f'Evaluating epoch: {epoch:04d}')\n",
    "\n",
    "    for data in test_loader:\n",
    "        \n",
    "        #data.edge_attr *= 0.5 #torch.rand(data.edge_attr.shape) #* (1.0 - 0.5)\n",
    "        # sum edges again to restart the data\n",
    "        row, col = data.edge_index\n",
    "        data.x = scatter(data.edge_attr, col, dim_size=data.num_nodes, reduce='sum')\n",
    "\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "        for split in y_true.keys():\n",
    "            mask = data[f'{split}_mask']\n",
    "            y_true[split].append(data.y[mask].cpu())\n",
    "            y_pred[split].append(out[mask].cpu())\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': torch.cat(y_true['train'], dim=0),\n",
    "        'y_pred': torch.cat(y_pred['train'], dim=0),\n",
    "    })['rocauc']\n",
    "\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': torch.cat(y_true['valid'], dim=0),\n",
    "        'y_pred': torch.cat(y_pred['valid'], dim=0),\n",
    "    })['rocauc']\n",
    "\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': torch.cat(y_true['test'], dim=0),\n",
    "        'y_pred': torch.cat(y_pred['test'], dim=0),\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1, 29):\n",
    "    loss = train(epoch)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        train_rocauc, valid_rocauc, test_rocauc = test()\n",
    "        print(f'Loss: {loss:.4f}, Train: {train_rocauc:.4f}, '\n",
    "          f'Val: {valid_rocauc:.4f}, Test: {test_rocauc:.4f}')\n",
    "    else:\n",
    "        print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903314d5-7512-4e63-9968-37e9cde9ed6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d20cee-b027-45a9-9018-3107e5c0558b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2754, Train: 0.7463, Val: 0.7339, Test: 0.6979\n"
     ]
    }
   ],
   "source": [
    "print(f'Loss: {loss:.4f}, Train: {train_rocauc:.4f}, '\n",
    "          f'Val: {valid_rocauc:.4f}, Test: {test_rocauc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f085a2-7575-46a6-899a-b4deeef9a950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2754, Train: 0.7463, Val: 0.7339, Test: 0.6979\n"
     ]
    }
   ],
   "source": [
    "print(f'Loss: {loss:.4f}, Train: {train_rocauc:.4f}, '\n",
    "          f'Val: {valid_rocauc:.4f}, Test: {test_rocauc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f89bdde1-2a6d-43db-8553-625382db4611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outdir = '/data101/makinen/graph_fishnets/models/fishnet_nc_24_nlyr_14'\n",
    "\n",
    "torch.save(model.state_dict(), outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e213cd-b447-4259-b948-3bf2dd549108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeeperGCN(\n",
       "  (node_encoder): Linear(in_features=8, out_features=24, bias=True)\n",
       "  (edge_encoder): Linear(in_features=8, out_features=24, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-13): 14 x DeepGCNLayer(block=res+)\n",
       "  )\n",
       "  (lin): Linear(in_features=24, out_features=112, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeeperGCN(n_p=HIDDEN_CHANNELS, num_layers=NUM_LAYERS).to(device)\n",
    "model.load_state_dict(torch.load(outdir))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9940f-0a21-4295-9507-05a7e64b8531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
